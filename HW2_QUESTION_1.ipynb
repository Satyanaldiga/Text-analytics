{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 1\n",
        "\n",
        "**Q1: Use the dataset “INFO 617 Mental Health QA_LIWC.csv” to answer the following\n",
        "questions.**\n",
        "\n",
        "Preparation: convert the numeric variable “Received_Bonus_Yes_No” into a binary variable\n",
        "“Bonus” that takes a value 1 when “Received_Bonus_Yes_No” is greater than or equal to 1 and 0\n",
        "otherwise.\n",
        "\n",
        "Build a CNN (Convolutional Nueral Network) that predicts the binary variable “Bonus” using\n",
        "texts in the column “Answer_English.” You can use the tutorial we discussed in class as a\n",
        "reference (INFO_617_Week_11_CNN_for_Text_Classification_(Chris_Tran).ipynb). However,\n",
        "since “Bonus” has an unbalanced class distribution (i.e., the majority of the answers did not\n",
        "receive a bonus), you might want to resample the texts before developing the CNN model.\n",
        "\n",
        "Before the resampling, you should split the dataset into a training set, a validation set, and a test\n",
        "set. Report your model’s performance on the test set using four metrics, accuracy, precision,\n",
        "recall, and F1-score. You can experiment with different hyper-parameter settings and pick a well-\n",
        "performing one to use in your model. However, no formal hyper-parameter tuning is required."
      ],
      "metadata": {
        "id": "_m_FL6gF4NYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing essential libraries for text preprocessing, visualization, and deep learning model building.\n",
        "This setup includes tools for data manipulation (pandas, numpy), NLP (nltk), progress tracking (tqdm),\n",
        " plotting (matplotlib), and building neural networks (PyTorch).\n"
      ],
      "metadata": {
        "id": "4Fkcr0-34Z8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fItUYFYprNNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df9e556-4dcb-49d6-da2c-8b3936f24d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Google Colab integration\n",
        "import os\n",
        "from google.colab import drive\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# For data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSaXS3c2r0EN",
        "outputId": "2107ee23-0a63-445c-c139-fff57746894c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import data as dataframe\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/INFO 617 Mental Health QA_LIWC (1).csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# calling head() method\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "mfGsZTMVr1P9",
        "outputId": "69172aee-aaf5-4f17-e4e6-a3a186308d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         QID                                   Question_English  \\\n",
              "0  100788023  Watching the elderly matchmaking program was v...   \n",
              "1  100788009  Falling in love with someone else and breaking...   \n",
              "2  100788013  I have been suffering from insomnia for half a...   \n",
              "3  100788013  I have been suffering from insomnia for half a...   \n",
              "4  100788013  I have been suffering from insomnia for half a...   \n",
              "\n",
              "                                      Answer_English  Usefulness_vote  \\\n",
              "0  Hello. Let's start by talking about young peop...                2   \n",
              "1  Hello! Sending you a virtual hug first. Thumbs...                2   \n",
              "2  Hello! ☆ I saw the issue you raised about inso...                8   \n",
              "3  Hello! ☆ I saw the issue you raised about inso...                8   \n",
              "4  Hi, I just want to hug you. I can feel the kin...                6   \n",
              "\n",
              "   Comment_count  Received_Bonus_Yes_No  Segment   WC  Analytic  Clout  ...  \\\n",
              "0              0                      0        1  359     52.91  83.28  ...   \n",
              "1              0                      0        1  199     72.83  96.90  ...   \n",
              "2              2                      1        1  611     37.89  75.35  ...   \n",
              "3              2                      0        1  611     37.89  75.35  ...   \n",
              "4              0                      0        1  587     82.45  46.10  ...   \n",
              "\n",
              "   nonflu  filler  AllPunc  Period  Comma  QMark  Exclam  Apostro  OtherP  \\\n",
              "0     0.0     0.0    18.94    6.96   8.36   0.84    0.28     2.23    0.28   \n",
              "1     0.0     0.0    12.06    5.03   5.03   0.00    0.50     0.50    1.01   \n",
              "2     0.0     0.0    15.71    4.75   7.36   0.65    0.16     1.80    0.98   \n",
              "3     0.0     0.0    15.71    4.75   7.36   0.65    0.16     1.80    0.98   \n",
              "4     0.0     0.0    16.18    6.30   4.94   1.19    0.17     0.68    2.90   \n",
              "\n",
              "   Emoji  \n",
              "0    0.0  \n",
              "1    0.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    0.0  \n",
              "\n",
              "[5 rows x 125 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a09f8088-5b65-4bbe-9fcc-17e24aca80e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QID</th>\n",
              "      <th>Question_English</th>\n",
              "      <th>Answer_English</th>\n",
              "      <th>Usefulness_vote</th>\n",
              "      <th>Comment_count</th>\n",
              "      <th>Received_Bonus_Yes_No</th>\n",
              "      <th>Segment</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>...</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>OtherP</th>\n",
              "      <th>Emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100788023</td>\n",
              "      <td>Watching the elderly matchmaking program was v...</td>\n",
              "      <td>Hello. Let's start by talking about young peop...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>359</td>\n",
              "      <td>52.91</td>\n",
              "      <td>83.28</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.94</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.36</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.23</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100788009</td>\n",
              "      <td>Falling in love with someone else and breaking...</td>\n",
              "      <td>Hello! Sending you a virtual hug first. Thumbs...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>199</td>\n",
              "      <td>72.83</td>\n",
              "      <td>96.90</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.06</td>\n",
              "      <td>5.03</td>\n",
              "      <td>5.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100788013</td>\n",
              "      <td>I have been suffering from insomnia for half a...</td>\n",
              "      <td>Hello! ☆ I saw the issue you raised about inso...</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>611</td>\n",
              "      <td>37.89</td>\n",
              "      <td>75.35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.71</td>\n",
              "      <td>4.75</td>\n",
              "      <td>7.36</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100788013</td>\n",
              "      <td>I have been suffering from insomnia for half a...</td>\n",
              "      <td>Hello! ☆ I saw the issue you raised about inso...</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>611</td>\n",
              "      <td>37.89</td>\n",
              "      <td>75.35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.71</td>\n",
              "      <td>4.75</td>\n",
              "      <td>7.36</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100788013</td>\n",
              "      <td>I have been suffering from insomnia for half a...</td>\n",
              "      <td>Hi, I just want to hug you. I can feel the kin...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>587</td>\n",
              "      <td>82.45</td>\n",
              "      <td>46.10</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.18</td>\n",
              "      <td>6.30</td>\n",
              "      <td>4.94</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.68</td>\n",
              "      <td>2.90</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 125 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a09f8088-5b65-4bbe-9fcc-17e24aca80e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a09f8088-5b65-4bbe-9fcc-17e24aca80e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a09f8088-5b65-4bbe-9fcc-17e24aca80e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b43a0888-7a05-4197-a696-b70a11c7a1ff\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b43a0888-7a05-4197-a696-b70a11c7a1ff')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b43a0888-7a05-4197-a696-b70a11c7a1ff button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "woFYKY0wsGlG",
        "outputId": "a1cf2878-eb68-4cb1-b549-a5798500b311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                QID  Usefulness_vote  Comment_count  Received_Bonus_Yes_No  \\\n",
              "count  8.639700e+04     86397.000000   86397.000000           86397.000000   \n",
              "mean   1.008195e+08         3.696066       0.188108               0.226779   \n",
              "std    1.494921e+04         2.642346       0.800155               0.431439   \n",
              "min    1.007880e+08         0.000000       0.000000               0.000000   \n",
              "25%    1.008065e+08         2.000000       0.000000               0.000000   \n",
              "50%    1.008258e+08         3.000000       0.000000               0.000000   \n",
              "75%    1.008321e+08         5.000000       0.000000               0.000000   \n",
              "max    1.008396e+08        31.000000      35.000000               4.000000   \n",
              "\n",
              "       Segment            WC      Analytic         Clout     Authentic  \\\n",
              "count  86397.0  86397.000000  86397.000000  86359.000000  86385.000000   \n",
              "mean       1.0    361.485677     35.420297     86.609288     37.184117   \n",
              "std        0.0    212.424732     20.612153     20.235013     24.168196   \n",
              "min        1.0      5.000000      1.000000      1.000000      1.000000   \n",
              "25%        1.0    215.000000     19.120000     84.020000     17.610000   \n",
              "50%        1.0    317.000000     32.440000     95.800000     33.740000   \n",
              "75%        1.0    469.000000     49.260000     99.000000     53.500000   \n",
              "max        1.0   3490.000000     99.000000     99.000000     99.000000   \n",
              "\n",
              "               Tone  ...        nonflu        filler       AllPunc  \\\n",
              "count  86055.000000  ...  86397.000000  86397.000000  86397.000000   \n",
              "mean      54.919290  ...      0.002742      0.000435     16.942721   \n",
              "std       31.029913  ...      0.057104      0.018663      3.244678   \n",
              "min        1.000000  ...      0.000000      0.000000      3.030000   \n",
              "25%       28.140000  ...      0.000000      0.000000     14.810000   \n",
              "50%       57.270000  ...      0.000000      0.000000     16.600000   \n",
              "75%       83.330000  ...      0.000000      0.000000     18.640000   \n",
              "max       99.000000  ...      5.880000      2.880000     68.420000   \n",
              "\n",
              "             Period         Comma         QMark        Exclam       Apostro  \\\n",
              "count  86397.000000  86397.000000  86397.000000  86397.000000  86397.000000   \n",
              "mean       5.668207      6.972443      0.473949      0.280939      1.671973   \n",
              "std        1.499754      1.862775      0.845861      0.518138      1.325822   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        4.750000      5.800000      0.000000      0.000000      0.750000   \n",
              "50%        5.530000      6.910000      0.000000      0.000000      1.430000   \n",
              "75%        6.430000      8.080000      0.640000      0.390000      2.300000   \n",
              "max       40.630000     26.670000     20.000000     31.030000     26.320000   \n",
              "\n",
              "             OtherP         Emoji  \n",
              "count  86397.000000  86397.000000  \n",
              "mean       1.875118      0.246987  \n",
              "std        1.924474      1.114994  \n",
              "min        0.000000      0.000000  \n",
              "25%        0.560000      0.000000  \n",
              "50%        1.380000      0.000000  \n",
              "75%        2.620000      0.000000  \n",
              "max       38.100000    160.000000  \n",
              "\n",
              "[8 rows x 123 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec32e369-f8f2-47d4-bcdb-b5d1fcadea5c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QID</th>\n",
              "      <th>Usefulness_vote</th>\n",
              "      <th>Comment_count</th>\n",
              "      <th>Received_Bonus_Yes_No</th>\n",
              "      <th>Segment</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>...</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>OtherP</th>\n",
              "      <th>Emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8.639700e+04</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.0</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86359.000000</td>\n",
              "      <td>86385.000000</td>\n",
              "      <td>86055.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "      <td>86397.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.008195e+08</td>\n",
              "      <td>3.696066</td>\n",
              "      <td>0.188108</td>\n",
              "      <td>0.226779</td>\n",
              "      <td>1.0</td>\n",
              "      <td>361.485677</td>\n",
              "      <td>35.420297</td>\n",
              "      <td>86.609288</td>\n",
              "      <td>37.184117</td>\n",
              "      <td>54.919290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002742</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>16.942721</td>\n",
              "      <td>5.668207</td>\n",
              "      <td>6.972443</td>\n",
              "      <td>0.473949</td>\n",
              "      <td>0.280939</td>\n",
              "      <td>1.671973</td>\n",
              "      <td>1.875118</td>\n",
              "      <td>0.246987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.494921e+04</td>\n",
              "      <td>2.642346</td>\n",
              "      <td>0.800155</td>\n",
              "      <td>0.431439</td>\n",
              "      <td>0.0</td>\n",
              "      <td>212.424732</td>\n",
              "      <td>20.612153</td>\n",
              "      <td>20.235013</td>\n",
              "      <td>24.168196</td>\n",
              "      <td>31.029913</td>\n",
              "      <td>...</td>\n",
              "      <td>0.057104</td>\n",
              "      <td>0.018663</td>\n",
              "      <td>3.244678</td>\n",
              "      <td>1.499754</td>\n",
              "      <td>1.862775</td>\n",
              "      <td>0.845861</td>\n",
              "      <td>0.518138</td>\n",
              "      <td>1.325822</td>\n",
              "      <td>1.924474</td>\n",
              "      <td>1.114994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.007880e+08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.030000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.008065e+08</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>215.000000</td>\n",
              "      <td>19.120000</td>\n",
              "      <td>84.020000</td>\n",
              "      <td>17.610000</td>\n",
              "      <td>28.140000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.810000</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.008258e+08</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>317.000000</td>\n",
              "      <td>32.440000</td>\n",
              "      <td>95.800000</td>\n",
              "      <td>33.740000</td>\n",
              "      <td>57.270000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.600000</td>\n",
              "      <td>5.530000</td>\n",
              "      <td>6.910000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.430000</td>\n",
              "      <td>1.380000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.008321e+08</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>469.000000</td>\n",
              "      <td>49.260000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>53.500000</td>\n",
              "      <td>83.330000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18.640000</td>\n",
              "      <td>6.430000</td>\n",
              "      <td>8.080000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>2.620000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.008396e+08</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3490.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>2.880000</td>\n",
              "      <td>68.420000</td>\n",
              "      <td>40.630000</td>\n",
              "      <td>26.670000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>31.030000</td>\n",
              "      <td>26.320000</td>\n",
              "      <td>38.100000</td>\n",
              "      <td>160.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 123 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec32e369-f8f2-47d4-bcdb-b5d1fcadea5c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec32e369-f8f2-47d4-bcdb-b5d1fcadea5c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec32e369-f8f2-47d4-bcdb-b5d1fcadea5c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-abb326bc-69a9-481f-b3e0-519f778f693b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-abb326bc-69a9-481f-b3e0-519f778f693b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-abb326bc-69a9-481f-b3e0-519f778f693b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Convert 'Received_Bonus_Yes_No' into a binary 'Bonus' column:\n",
        " 1 if bonus was received (value ≥ 1), else 0\n"
      ],
      "metadata": {
        "id": "w1xDExUI4hne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Bonus'] = (df['Received_Bonus_Yes_No'] >= 1).astype(int)"
      ],
      "metadata": {
        "id": "ysEuJaXMsJ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into train (80%), validation (10%), and test (10%) with stratification on 'Bonus'\n",
        "Downsample the majority class in the training set to address class imbalance\n",
        " Save all splits for later use\n"
      ],
      "metadata": {
        "id": "akRoE1J94odO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Bonus\"])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"Bonus\"])\n",
        "\n",
        "# 4. Downsample ONLY the training set\n",
        "train_majority = train_df[train_df[\"Bonus\"] == 0]\n",
        "train_minority = train_df[train_df[\"Bonus\"] == 1]\n",
        "\n",
        "train_majority_downsampled = resample(train_majority,\n",
        "                                      replace=False,\n",
        "                                      n_samples=len(train_minority),\n",
        "                                      random_state=42)\n",
        "\n",
        "train_balanced = pd.concat([train_majority_downsampled, train_minority])\n",
        "train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 5. Save for reference\n",
        "train_balanced.to_csv(\"Train_Balanced.csv\", index=False)\n",
        "val_df.to_csv(\"Val_Original.csv\", index=False)\n",
        "test_df.to_csv(\"Test_Original.csv\", index=False)\n",
        "\n",
        "print(\" Step 1 complete.\")\n",
        "print(\"→ Train set balanced.\")\n",
        "print(\"→ Val/Test sets keep original class distribution.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWViLlVGsMhc",
        "outputId": "e07861ff-996c-46ae-f855-70e06b8a46ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Step 1 complete.\n",
            "→ Train set balanced.\n",
            "→ Val/Test sets keep original class distribution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: Preprocess text by tokenizing, building a vocabulary, and encoding input sequences.\n",
        " Uses NLTK's word_tokenize to split text into tokens, builds a word2idx mapping,\n",
        " and pads/truncates sequences to ensure uniform length across train/val/test sets.\n"
      ],
      "metadata": {
        "id": "A_N4t-Ua4vVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Tokenization, Vocab Building, Encoding\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 1. Load data from step 1\n",
        "train_df = pd.read_csv(\"Train_Balanced.csv\")\n",
        "val_df = pd.read_csv(\"Val_Original.csv\")\n",
        "test_df = pd.read_csv(\"Test_Original.csv\")\n",
        "\n",
        "# 2. Extract full text and labels\n",
        "train_texts = train_df[\"Answer_English\"].astype(str).tolist()\n",
        "val_texts = val_df[\"Answer_English\"].astype(str).tolist()\n",
        "test_texts = test_df[\"Answer_English\"].astype(str).tolist()\n",
        "\n",
        "train_labels = train_df[\"Bonus\"].tolist()\n",
        "val_labels = val_df[\"Bonus\"].tolist()\n",
        "test_labels = test_df[\"Bonus\"].tolist()\n",
        "\n",
        "# 3. Tokenization + Vocabulary builder\n",
        "def tokenize(texts):\n",
        "    tokenized_texts = []\n",
        "    word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    idx = 2\n",
        "    max_len = 0\n",
        "\n",
        "    for sent in texts:\n",
        "        tokens = word_tokenize(sent.lower())\n",
        "        tokenized_texts.append(tokens)\n",
        "        max_len = max(max_len, len(tokens))\n",
        "\n",
        "        for token in tokens:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len\n",
        "\n",
        "# 4. Encode and pad the texts\n",
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokens in tokenized_texts:\n",
        "        tokens = tokens[:max_len] + ['<pad>'] * max(0, max_len - len(tokens))\n",
        "        input_ids.append([word2idx.get(token, word2idx[\"<unk>\"]) for token in tokens])\n",
        "    return np.array(input_ids)\n",
        "\n",
        "# 5. Tokenize ONLY training set to build vocab\n",
        "train_tokenized, word2idx, max_len = tokenize(train_texts)\n",
        "\n",
        "# 6. Tokenize and encode all sets using same vocab + max_len\n",
        "val_tokenized = [word_tokenize(text.lower()) for text in val_texts]\n",
        "test_tokenized = [word_tokenize(text.lower()) for text in test_texts]\n",
        "\n",
        "train_input_ids = encode(train_tokenized, word2idx, max_len)\n",
        "val_input_ids = encode(val_tokenized, word2idx, max_len)\n",
        "test_input_ids = encode(test_tokenized, word2idx, max_len)\n",
        "\n",
        "print(\" Tokenization and encoding complete.\")\n",
        "print(f\" Vocab size: {len(word2idx)} | Max sequence length: {max_len}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHy5bNLBsNfV",
        "outputId": "27c1c879-9843-4fb8-bb7a-8fddb919e791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokenization and encoding complete.\n",
            " Vocab size: 41555 | Max sequence length: 2803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: Convert encoded sequences and labels into PyTorch tensors.\n",
        " Wrap them into TensorDatasets and load them into DataLoaders for efficient batching during training and evaluation.\n"
      ],
      "metadata": {
        "id": "JRQA-EIc403a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert to tensors\n",
        "train_inputs = torch.tensor(train_input_ids)\n",
        "val_inputs = torch.tensor(val_input_ids)\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "val_data = TensorDataset(val_inputs, val_labels)\n",
        "test_data = TensorDataset(test_inputs, test_labels)\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# DataLoaders\n",
        "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)\n",
        "\n",
        "print(\" DataLoaders ready.\")\n",
        "print(f\"Train: {len(train_dataloader)} batches | Val: {len(val_dataloader)} | Test: {len(test_dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kYrRl5msRH6",
        "outputId": "1fffa7eb-3472-44ff-9c91-7d044e143bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DataLoaders ready.\n",
            "Train: 613 batches | Val: 173 | Test: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4: Download GloVe pretrained word vectors (300d) and extract them.\n",
        "Then, build an embedding matrix aligned with the model's vocabulary (word2idx),\n",
        " initializing missing words with random vectors and padding token with zeros.\n"
      ],
      "metadata": {
        "id": "N9delK5V46c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# === STEP 1: Download and extract GloVe === #\n",
        "EMBED_DIR = \"glove\"\n",
        "os.makedirs(EMBED_DIR, exist_ok=True)\n",
        "\n",
        "GLOVE_URL = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "GLOVE_ZIP_PATH = os.path.join(EMBED_DIR, \"glove.6B.zip\")\n",
        "\n",
        "# Download if not already present\n",
        "if not os.path.exists(GLOVE_ZIP_PATH):\n",
        "    print(\"Downloading GloVe vectors...\")\n",
        "    response = requests.get(GLOVE_URL, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024\n",
        "    with open(GLOVE_ZIP_PATH, 'wb') as f:\n",
        "        for data in tqdm(response.iter_content(block_size), total=total_size // block_size, unit='KB', unit_scale=True):\n",
        "            f.write(data)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# Extract if not already extracted\n",
        "glove_txt_path = os.path.join(EMBED_DIR, \"glove.6B.300d.txt\")\n",
        "if not os.path.exists(glove_txt_path):\n",
        "    print(\"Extracting GloVe vectors...\")\n",
        "    with zipfile.ZipFile(GLOVE_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EMBED_DIR)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "# === STEP 2: Build the embedding matrix === #\n",
        "def load_glove_embeddings(word2idx, embedding_path, embed_dim=300):\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), embed_dim))\n",
        "    embeddings[word2idx.get('<pad>', 0)] = np.zeros(embed_dim)\n",
        "    found = 0\n",
        "    with open(embedding_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            if word in word2idx:\n",
        "                embeddings[word2idx[word]] = vector\n",
        "                found += 1\n",
        "    print(f\"Found {found}/{len(word2idx)} words in GloVe.\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "embedding_matrix = load_glove_embeddings(word2idx, glove_txt_path)\n",
        "\n",
        "print(\" Embedding matrix ready.\")\n",
        "print(f\"Shape: {embedding_matrix.shape}\")\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4TMnLnMs5Yu",
        "outputId": "198bbeb3-b836-418b-92a8-51afbe36828c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "842kKB [02:38, 5.31kKB/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "Extracting GloVe vectors...\n",
            "Extraction complete.\n",
            "Loading GloVe embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:27, 14480.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28099/41555 words in GloVe.\n",
            " Embedding matrix ready.\n",
            "Shape: (41555, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: Define the CNN architecture for text classification.\n",
        " This model uses multiple convolutional filters to capture n-gram features,\n",
        " followed by max-pooling, dropout for regularization, and a fully connected layer for classification.\n",
        " Supports both random and pretrained embeddings (GloVe).\n"
      ],
      "metadata": {
        "id": "cK9kHnga5BTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_NLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "        super(CNN_NLP, self).__init__()\n",
        "\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32),\n",
        "                freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(\n",
        "                num_embeddings=vocab_size,\n",
        "                embedding_dim=self.embed_dim,\n",
        "                padding_idx=0,\n",
        "                max_norm=5.0)\n",
        "\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "Ln-5UhYQs8Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: Initialize the CNN model with given hyperparameters.\n",
        " Supports both pretrained and random embeddings. Uses Adadelta optimizer for training.\n"
      ],
      "metadata": {
        "id": "UBSkCvkh5Gu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def init_model(pretrained_embedding=None,\n",
        "               freeze_embedding=False,\n",
        "               vocab_size=None,\n",
        "               embed_dim=300,\n",
        "               filter_sizes=[3, 4, 5],\n",
        "               num_filters=[100, 100, 100],\n",
        "               num_classes=2,\n",
        "               dropout=0.5,\n",
        "               learning_rate=0.25):\n",
        "\n",
        "    model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
        "                    freeze_embedding=freeze_embedding,\n",
        "                    vocab_size=vocab_size,\n",
        "                    embed_dim=embed_dim,\n",
        "                    filter_sizes=filter_sizes,\n",
        "                    num_filters=num_filters,\n",
        "                    num_classes=num_classes,\n",
        "                    dropout=dropout)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, rho=0.95)\n",
        "\n",
        "    return model, optimizer\n"
      ],
      "metadata": {
        "id": "yQ-LTAMQtxNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7: Set the device to GPU if available, otherwise default to CPU\n"
      ],
      "metadata": {
        "id": "T6whU6vO5MrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3iZ41bVtzLk",
        "outputId": "81874927-32b7-4f7a-9e9d-37b8b33b753e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 8: Define training and evaluation loops.\n",
        " The `train()` function runs the training process across multiple epochs, logging loss and accuracy.\n",
        " The `evaluate()` function computes average loss and accuracy on validation/test datasets without updating model weights.\n"
      ],
      "metadata": {
        "id": "0_-_Xuoj5QJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "    best_val_acc = 0\n",
        "\n",
        "    print(\" Training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Time':^9}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        t0 = time.time()\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            b_input_ids, b_labels = [t.to(device) for t in batch]\n",
        "            model.zero_grad()\n",
        "\n",
        "            logits = model(b_input_ids)\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # Validate\n",
        "        if val_dataloader:\n",
        "            val_loss, val_acc = evaluate(model, val_dataloader)\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "            elapsed = time.time() - t0\n",
        "            print(f\"{epoch + 1:^7} | {avg_train_loss:^12.4f} | {val_loss:^10.4f} | {val_acc:^9.2f} | {elapsed:^9.2f}\")\n",
        "\n",
        "    print(\"\\n Training complete! Best val accuracy:\", round(best_val_acc, 2), \"%\")\n",
        "\n",
        "# Evaluation loop (validation or test)\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            b_input_ids, b_labels = [t.to(device) for t in batch]\n",
        "            logits = model(b_input_ids)\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            acc = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "            val_acc.append(acc)\n",
        "\n",
        "    return np.mean(val_loss), np.mean(val_acc)\n"
      ],
      "metadata": {
        "id": "v_YHTk_Ot0vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 9: Train three CNN model variants:\n",
        " 1. CNN-rand: Uses randomly initialized embeddings\n",
        " 2. CNN-static: Uses pretrained GloVe embeddings (frozen)\n",
        " 3. CNN-non-static: Uses pretrained GloVe embeddings (fine-tuned during training)\n"
      ],
      "metadata": {
        "id": "CtlKXYEy5UwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-rand: randomly initialized embeddings\n",
        "print(\"\\n Training CNN-rand (random embeddings)\")\n",
        "cnn_rand, opt_rand = init_model(vocab_size=len(word2idx))\n",
        "train(cnn_rand, opt_rand, train_dataloader, val_dataloader, epochs=10)\n",
        "\n",
        "# CNN-static: pretrained fastText, frozen\n",
        "print(\"\\n Training CNN-static (pretrained, frozen)\")\n",
        "cnn_static, opt_static = init_model(pretrained_embedding=embedding_matrix,\n",
        "                                    freeze_embedding=True)\n",
        "train(cnn_static, opt_static, train_dataloader, val_dataloader, epochs=10)\n",
        "\n",
        "# CNN-non-static: pretrained fastText, fine-tuned\n",
        "print(\"\\n Training CNN-non-static (pretrained, trainable)\")\n",
        "cnn_non_static, opt_non_static = init_model(pretrained_embedding=embedding_matrix,\n",
        "                                            freeze_embedding=False)\n",
        "train(cnn_non_static, opt_non_static, train_dataloader, val_dataloader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ARpJ9w8t206",
        "outputId": "a3f2afd1-7ca2-43cf-96f4-b9435911399a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training CNN-rand (random embeddings)\n",
            " Training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |   Time   \n",
            "------------------------------------------------------------\n",
            "   1    |    0.6212    |   0.5995   |   67.86   |   88.61  \n",
            "   2    |    0.5529    |   0.5774   |   69.67   |   91.56  \n",
            "   3    |    0.5184    |   0.6334   |   65.91   |   93.72  \n",
            "   4    |    0.4930    |   0.4470   |   79.00   |   94.60  \n",
            "   5    |    0.4673    |   0.4865   |   76.03   |   94.98  \n",
            "   6    |    0.4428    |   0.4714   |   77.55   |   95.18  \n",
            "   7    |    0.4181    |   0.5986   |   70.81   |   95.39  \n",
            "   8    |    0.3929    |   0.4724   |   77.84   |   95.34  \n",
            "   9    |    0.3632    |   0.6011   |   71.98   |   95.46  \n",
            "  10    |    0.3408    |   0.4014   |   81.63   |   95.40  \n",
            "\n",
            " Training complete! Best val accuracy: 81.63 %\n",
            "\n",
            " Training CNN-static (pretrained, frozen)\n",
            " Training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |   Time   \n",
            "------------------------------------------------------------\n",
            "   1    |    0.5870    |   0.5505   |   71.84   |   70.91  \n",
            "   2    |    0.5216    |   0.7763   |   58.51   |   70.86  \n",
            "   3    |    0.4867    |   0.4966   |   76.12   |   70.65  \n",
            "   4    |    0.4692    |   0.5412   |   73.34   |   70.77  \n",
            "   5    |    0.4449    |   0.6304   |   68.60   |   70.55  \n",
            "   6    |    0.4238    |   0.5429   |   74.48   |   70.67  \n",
            "   7    |    0.4031    |   0.4953   |   77.07   |   70.52  \n",
            "   8    |    0.3811    |   0.4044   |   81.86   |   70.42  \n",
            "   9    |    0.3581    |   0.5379   |   75.51   |   70.67  \n",
            "  10    |    0.3398    |   0.5018   |   77.70   |   70.89  \n",
            "\n",
            " Training complete! Best val accuracy: 81.86 %\n",
            "\n",
            " Training CNN-non-static (pretrained, trainable)\n",
            " Training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |   Time   \n",
            "------------------------------------------------------------\n",
            "   1    |    0.5886    |   0.5370   |   73.11   |  100.02  \n",
            "   2    |    0.5208    |   0.5188   |   73.90   |  100.10  \n",
            "   3    |    0.4914    |   0.4973   |   75.91   |   99.93  \n",
            "   4    |    0.4651    |   0.6414   |   67.45   |   99.54  \n",
            "   5    |    0.4416    |   0.5261   |   74.99   |   99.73  \n",
            "   6    |    0.4198    |   0.6972   |   66.68   |   99.54  \n",
            "   7    |    0.3948    |   0.6785   |   69.18   |   99.59  \n",
            "   8    |    0.3718    |   0.5836   |   73.53   |   99.58  \n",
            "   9    |    0.3521    |   0.5476   |   75.51   |   99.99  \n",
            "  10    |    0.3251    |   0.6121   |   73.76   |  100.05  \n",
            "\n",
            " Training complete! Best val accuracy: 75.91 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 10: Training Results Summary\n",
        " CNN-rand (random embeddings) achieved best val accuracy: **81.63%**# CNN-static (pretrained, frozen) achieved slightly higher accuracy: **81.86%**\n",
        " CNN-non-static (pretrained, trainable) peaked at: **75.91%**\n",
        "\n",
        "# Takeaway:\n",
        " - Pretrained embeddings (GloVe) provided a solid starting point.\n",
        " - Freezing the embeddings (static) performed slightly better than fine-tuning them (non-static),\n",
        "   likely due to overfitting or sensitivity to training data.\n",
        " - Random initialization (CNN-rand) performed competitively, showing the strength of the architecture alone.\n"
      ],
      "metadata": {
        "id": "ZnI_j-qD5czN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_on_test(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            b_input_ids, b_labels = [t.to(device) for t in batch]\n",
        "            logits = model(b_input_ids)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds)\n",
        "    rec = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    return acc, prec, rec, f1\n"
      ],
      "metadata": {
        "id": "TAOORCV44BWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 11: Evaluate all trained models on the test set using accuracy, precision, recall, and F1-score.\n",
        " Compare how each variant (random, static, non-static) generalizes to unseen data.\n"
      ],
      "metadata": {
        "id": "yhtHvNO35lb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    (\"CNN-rand\", cnn_rand),\n",
        "    (\"CNN-static\", cnn_static),\n",
        "    (\"CNN-non-static\", cnn_non_static)\n",
        "]\n",
        "\n",
        "for name, model in models:\n",
        "    acc, prec, rec, f1 = evaluate_on_test(model, test_dataloader)\n",
        "    print(f\"{name} Test Set Evaluation:\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31hUOxSS4D8m",
        "outputId": "2c5f7182-fb3e-4d25-8104-52d299065962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN-rand Test Set Evaluation:\n",
            "Accuracy:  0.8102\n",
            "Precision: 0.5612\n",
            "Recall:    0.6580\n",
            "F1-score:  0.6058\n",
            "----------------------------------------\n",
            "CNN-static Test Set Evaluation:\n",
            "Accuracy:  0.7657\n",
            "Precision: 0.4824\n",
            "Recall:    0.7802\n",
            "F1-score:  0.5962\n",
            "----------------------------------------\n",
            "CNN-non-static Test Set Evaluation:\n",
            "Accuracy:  0.7257\n",
            "Precision: 0.4395\n",
            "Recall:    0.8632\n",
            "F1-score:  0.5825\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 12: Test Set Performance Summary\n",
        "\n",
        "#  CNN-rand\n",
        " - Best overall **accuracy** (81.02%) and **F1-score** (0.6058)\n",
        " - Balanced performance, strong generalization despite no pretrained knowledge\n",
        "\n",
        "#  CNN-static\n",
        " - Highest **recall** among the pretrained models (78.02%)\n",
        "- Performs well at catching positive cases, but lower precision\n",
        "\n",
        "#  CNN-non-static\n",
        " - **Highest recall** (86.32%), but with a **lower precision** (43.95%)\n",
        " - Tends to over-predict positives, resulting in lower overall accuracy\n",
        "\n",
        "# Takeaway:\n",
        " - Random embeddings surprisingly performed best overall.\n",
        " - Fine-tuning pretrained embeddings (non-static) increased recall at the cost of precision.\n",
        " - Freezing pretrained embeddings (static) gave balanced but slightly underperforming results.\n"
      ],
      "metadata": {
        "id": "lCRWa8Vp5rL_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_F2oKQE-4MHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}