{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Consider the same predict task in Q1. Now, built another prediction model using the LSTM\n",
        "(Long Short-Term Memory) neural network for the same task. You should continue to use the\n",
        "resampled dataset in Q1. You can use the tutorial\n",
        "(INFO_617_Week_11_LSTM_for_Text_Classification.ipynb) as a reference. First, built a\n",
        "prediction model that uses unidirectional LSTM layer(s). Then, change the LSTM setting to bi-\n",
        "directional and replicate the training and evaluation process.\n",
        "You can experiment with different hyper-parameter settings and pick a well-performing one to\n",
        "use. No formal hyper-parameter tuning is required. Compare the performance of the\n",
        "unidirectional LSTM and bi-directional LSTM prediction models using four metrics – accuracy,\n",
        "precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "j_TlKvwDDru9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import essential libraries for reproducibility, model building (PyTorch),\n",
        " evaluation (sklearn), and numerical operations (NumPy).\n",
        "Ensures consistent results and provides tools for training and reporting performance.\n"
      ],
      "metadata": {
        "id": "wdYCXnMzD7TU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57QOHpJ87FXG"
      },
      "outputs": [],
      "source": [
        "#Imports & Reproducibility\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Google Colab integration\n",
        "import os\n",
        "from google.colab import drive\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# For data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCfdyRNy7MQW",
        "outputId": "1456248f-99c7-44e3-e07b-f3f944d9b489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import data as dataframe\n",
        "file_path = '/content/drive/MyDrive/text/INFO 617 Mental Health QA_LIWC (1).csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "gOSCDw-u7Vp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling head() method\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wWm2LMy17Y4h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "ae451cec-3a0c-49f9-d9cd-7feb2036b108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         QID                                   Question_English  \\\n",
              "0  100788023  Watching the elderly matchmaking program was v...   \n",
              "1  100788009  Falling in love with someone else and breaking...   \n",
              "2  100788013  I have been suffering from insomnia for half a...   \n",
              "3  100788013  I have been suffering from insomnia for half a...   \n",
              "4  100788013  I have been suffering from insomnia for half a...   \n",
              "\n",
              "                                      Answer_English  Usefulness_vote  \\\n",
              "0  Hello. Let's start by talking about young peop...                2   \n",
              "1  Hello! Sending you a virtual hug first. Thumbs...                2   \n",
              "2  Hello! ☆ I saw the issue you raised about inso...                8   \n",
              "3  Hello! ☆ I saw the issue you raised about inso...                8   \n",
              "4  Hi, I just want to hug you. I can feel the kin...                6   \n",
              "\n",
              "   Comment_count  Received_Bonus_Yes_No  Segment   WC  Analytic  Clout  ...  \\\n",
              "0              0                      0        1  359     52.91  83.28  ...   \n",
              "1              0                      0        1  199     72.83  96.90  ...   \n",
              "2              2                      1        1  611     37.89  75.35  ...   \n",
              "3              2                      0        1  611     37.89  75.35  ...   \n",
              "4              0                      0        1  587     82.45  46.10  ...   \n",
              "\n",
              "   nonflu  filler  AllPunc  Period  Comma  QMark  Exclam  Apostro  OtherP  \\\n",
              "0     0.0     0.0    18.94    6.96   8.36   0.84    0.28     2.23    0.28   \n",
              "1     0.0     0.0    12.06    5.03   5.03   0.00    0.50     0.50    1.01   \n",
              "2     0.0     0.0    15.71    4.75   7.36   0.65    0.16     1.80    0.98   \n",
              "3     0.0     0.0    15.71    4.75   7.36   0.65    0.16     1.80    0.98   \n",
              "4     0.0     0.0    16.18    6.30   4.94   1.19    0.17     0.68    2.90   \n",
              "\n",
              "   Emoji  \n",
              "0    0.0  \n",
              "1    0.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    0.0  \n",
              "\n",
              "[5 rows x 125 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf9c53d5-e29d-49b8-9b98-c84de4715677\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QID</th>\n",
              "      <th>Question_English</th>\n",
              "      <th>Answer_English</th>\n",
              "      <th>Usefulness_vote</th>\n",
              "      <th>Comment_count</th>\n",
              "      <th>Received_Bonus_Yes_No</th>\n",
              "      <th>Segment</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>...</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>OtherP</th>\n",
              "      <th>Emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100788023</td>\n",
              "      <td>Watching the elderly matchmaking program was v...</td>\n",
              "      <td>Hello. Let's start by talking about young peop...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>359</td>\n",
              "      <td>52.91</td>\n",
              "      <td>83.28</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.94</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.36</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.23</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100788009</td>\n",
              "      <td>Falling in love with someone else and breaking...</td>\n",
              "      <td>Hello! Sending you a virtual hug first. Thumbs...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>199</td>\n",
              "      <td>72.83</td>\n",
              "      <td>96.90</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.06</td>\n",
              "      <td>5.03</td>\n",
              "      <td>5.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100788013</td>\n",
              "      <td>I have been suffering from insomnia for half a...</td>\n",
              "      <td>Hello! ☆ I saw the issue you raised about inso...</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>611</td>\n",
              "      <td>37.89</td>\n",
              "      <td>75.35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.71</td>\n",
              "      <td>4.75</td>\n",
              "      <td>7.36</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100788013</td>\n",
              "      <td>I have been suffering from insomnia for half a...</td>\n",
              "      <td>Hello! ☆ I saw the issue you raised about inso...</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>611</td>\n",
              "      <td>37.89</td>\n",
              "      <td>75.35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.71</td>\n",
              "      <td>4.75</td>\n",
              "      <td>7.36</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100788013</td>\n",
              "      <td>I have been suffering from insomnia for half a...</td>\n",
              "      <td>Hi, I just want to hug you. I can feel the kin...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>587</td>\n",
              "      <td>82.45</td>\n",
              "      <td>46.10</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.18</td>\n",
              "      <td>6.30</td>\n",
              "      <td>4.94</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.68</td>\n",
              "      <td>2.90</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 125 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf9c53d5-e29d-49b8-9b98-c84de4715677')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf9c53d5-e29d-49b8-9b98-c84de4715677 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf9c53d5-e29d-49b8-9b98-c84de4715677');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24cc5002-bd0d-4caa-91a0-40205c49ce55\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24cc5002-bd0d-4caa-91a0-40205c49ce55')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24cc5002-bd0d-4caa-91a0-40205c49ce55 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create binary 'Bonus' column: 1 if bonus received (value ≥ 1), else 0\n"
      ],
      "metadata": {
        "id": "G2SY50LmEBT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Bonus'] = (df['Received_Bonus_Yes_No'] >= 1).astype(int)"
      ],
      "metadata": {
        "id": "LGvmoEUJ9Pbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Set random seed for reproducibility across NumPy, Python, and PyTorch (CPU & GPU)\n",
        " Also sets the device to GPU if available, else defaults to CPU\n"
      ],
      "metadata": {
        "id": "Bj2DlgcgEEdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5M3d1Uy9REZ",
        "outputId": "b4271e69-66ef-409c-8e44-d78f05761092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP: Train/Validation/Test Split and Downsampling\n",
        " 1. Split the dataset with stratified sampling to preserve class distribution.\n",
        " 2. Downsample the majority class (Bonus=0) in the training set to match the minority class (Bonus=1).\n",
        " 3. Combine and shuffle to create a balanced training dataset.\n",
        " 4. Print class distribution to confirm balance.\n"
      ],
      "metadata": {
        "id": "2PC7U3koEJml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Train/Val/Test Split with stratification\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['Bonus'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Bonus'], random_state=42)\n",
        "\n",
        "# 2. Downsample Bonus = 0 (majority) to match Bonus = 1 (minority)\n",
        "pos = train_df[train_df['Bonus'] == 1]\n",
        "neg = train_df[train_df['Bonus'] == 0]\n",
        "neg_downsampled = resample(neg, replace=True, n_samples=len(pos), random_state=42)\n",
        "\n",
        "# 3. Combine and shuffle\n",
        "train_balanced = pd.concat([pos, neg_downsampled]).sample(frac=1, random_state=42)\n",
        "\n",
        "# 4. Check balance\n",
        "print(\"Balanced train distribution:\\n\", Counter(train_balanced['Bonus']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4TT3Dos_pRH",
        "outputId": "0bb2cf4f-8d9b-4671-cc26-0bba268b28ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced train distribution:\n",
            " Counter({0: 15320, 1: 15320})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP: Text Tokenization and Padding\n",
        " 1. Use Keras Tokenizer to convert text into sequences with a fixed vocabulary size.\n",
        " 2. Pad all sequences to the same maximum length for input consistency.\n",
        " 3. Apply transformation to training, validation, and test sets.\n"
      ],
      "metadata": {
        "id": "Y52EjZBUEOOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set vocab size and max sequence length\n",
        "MAX_WORDS = 10000\n",
        "MAX_LEN = 100\n",
        "\n",
        "# Initialize and fit tokenizer on the balanced training set\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_balanced['Answer_English'])\n",
        "\n",
        "# Function to convert and pad sequences\n",
        "def seqs(df_):\n",
        "    sequences = tokenizer.texts_to_sequences(df_['Answer_English'])\n",
        "    return pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "\n",
        "# Apply to train, validation, and test\n",
        "X_train = seqs(train_balanced)\n",
        "X_val   = seqs(val_df)\n",
        "X_test  = seqs(test_df)\n",
        "\n",
        "y_train = train_balanced['Bonus'].values\n",
        "y_val   = val_df['Bonus'].values\n",
        "y_test  = test_df['Bonus'].values\n",
        "\n",
        "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGMsojkc_zFG",
        "outputId": "ef2964a3-4db1-40bd-86bc-0f14b8eeac34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (30640, 100) (8640, 100) (8640, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP: Load GloVe embeddings and build embedding matrix\n",
        " 1. Download and extract GloVe 300d vectors.\n",
        " 2. Create a word-to-vector dictionary from GloVe.\n",
        " 3. Initialize an embedding matrix aligned with tokenizer’s word index.\n",
        " 4. Convert to PyTorch tensor for model use.\n"
      ],
      "metadata": {
        "id": "6b7jtx3YESor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe 300d\n",
        "!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip glove.6B.300d.txt\n",
        "\n",
        "# Set embedding dimensions\n",
        "EMBED_DIM = 300\n",
        "word2idx = tokenizer.word_index\n",
        "vocab_size = min(MAX_WORDS, len(word2idx)) + 1\n",
        "\n",
        "# Load GloVe into a dictionary\n",
        "emb_index = {}\n",
        "with open('glove.6B.300d.txt', 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        emb_index[word] = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# Build embedding matrix\n",
        "embedding_tensor = np.zeros((vocab_size, EMBED_DIM))\n",
        "for word, idx in word2idx.items():\n",
        "    if idx < vocab_size and word in emb_index:\n",
        "        embedding_tensor[idx] = emb_index[word]\n",
        "\n",
        "embedding_tensor = torch.tensor(embedding_tensor, dtype=torch.float32)\n",
        "print(\"Embedding matrix shape:\", embedding_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCGV3HDj_7zb",
        "outputId": "31d8f3c3-3941-40d9-f49a-a83cd27f14f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: torch.Size([10001, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP: Convert padded sequences and labels into PyTorch Datasets and DataLoaders\n",
        "Enables efficient batching and shuffling for training and evaluation\n"
      ],
      "metadata": {
        "id": "56ZuChzFEX4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  # Create TensorDatasets\n",
        "  train_ds = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
        "  val_ds   = TensorDataset(torch.LongTensor(X_val),   torch.LongTensor(y_val))\n",
        "  test_ds  = TensorDataset(torch.LongTensor(X_test),  torch.LongTensor(y_test))\n",
        "\n",
        "  # Create DataLoaders\n",
        "  train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "  test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "vluSOISuAwhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP: Define the LSTM-based text classification model\n",
        " Supports both unidirectional and bidirectional LSTMs, with optional pretrained embeddings.\n",
        " The final hidden state is passed through a dropout and a fully connected layer for binary classification.\n"
      ],
      "metadata": {
        "id": "GKDUxFraEd_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_NLP(nn.Module):\n",
        "    \"\"\"LSTM model for text classification.\"\"\"\n",
        "    def __init__(self,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers=2,\n",
        "                 bidirectional=False,\n",
        "                 lstm_dropout=0.5,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 fc_dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                pretrained_embedding, freeze=freeze_embedding)\n",
        "            self.embed_dim = pretrained_embedding.size(1)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(vocab_size, self.embed_dim, padding_idx=0, max_norm=5.0)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=self.embed_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=bidirectional,\n",
        "                            dropout=lstm_dropout,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(fc_dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedded = self.embedding(input_ids).float()  # shape: (B, L, E)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        if self.lstm.bidirectional:\n",
        "            # concatenate last hidden states of both directions\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1]\n",
        "        hidden = self.dropout(hidden)\n",
        "        return self.fc(hidden)\n"
      ],
      "metadata": {
        "id": "a9Fqtve_A0ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP: Initialize the LSTM model with specified architecture and training parameters\n",
        "Supports pretrained or randomly initialized embeddings and returns the model with Adadelta optimizer\n"
      ],
      "metadata": {
        "id": "kvO2xipsEiHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def initialize_lstm(pretrained_embedding=None,\n",
        "                    freeze_embedding=False,\n",
        "                    vocab_size=None,\n",
        "                    embed_dim=300,\n",
        "                    hidden_dim=128,\n",
        "                    output_dim=2,\n",
        "                    n_layers=2,\n",
        "                    bidirectional=False,\n",
        "                    lstm_dropout=0.3,\n",
        "                    fc_dropout=0.3,\n",
        "                    learning_rate=0.25):\n",
        "    model = LSTM_NLP(hidden_dim=hidden_dim,\n",
        "                     output_dim=output_dim,\n",
        "                     n_layers=n_layers,\n",
        "                     bidirectional=bidirectional,\n",
        "                     lstm_dropout=lstm_dropout,\n",
        "                     pretrained_embedding=pretrained_embedding,\n",
        "                     freeze_embedding=freeze_embedding,\n",
        "                     vocab_size=vocab_size,\n",
        "                     embed_dim=embed_dim,\n",
        "                     fc_dropout=fc_dropout).to(device)\n",
        "\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, rho=0.95)\n",
        "    return model, optimizer\n"
      ],
      "metadata": {
        "id": "Acsn8UdYA2Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP: Define training and evaluation loops for the LSTM model\n",
        " - `train_lstm()` trains the model for a given number of epochs, logging training/validation loss and accuracy\n",
        " - `evaluate()` calculates classification accuracy on any given dataset\n"
      ],
      "metadata": {
        "id": "8aKEIuoOEnuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_lstm(model, optimizer, train_loader, val_loader=None, epochs=10):\n",
        "    best_acc = 0.0\n",
        "    print(\"Epoch | Train Loss  | Val Loss   | Val Acc  | Time(s)\")\n",
        "    print(\"-\"*60)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        t0, train_loss = time.time(), 0.0\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss, correct = 0.0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    out = model(xb)\n",
        "                    val_loss += loss_fn(out, yb).item()\n",
        "                    preds = out.argmax(dim=1)\n",
        "                    correct += (preds == yb).sum().item()\n",
        "            val_loss /= len(val_loader)\n",
        "            val_acc = 100 * correct / len(val_loader.dataset)\n",
        "        else:\n",
        "            val_loss, val_acc = float('nan'), float('nan')\n",
        "\n",
        "        print(f\"{epoch:^5} | {train_loss:^10.4f} | {val_loss:^10.4f} | {val_acc:^8.2f} | {time.time()-t0:^7.2f}\")\n",
        "        best_acc = max(best_acc, val_acc)\n",
        "\n",
        "    print(f\"\\n Best Val Acc: {best_acc:.2f}%\\n\")\n"
      ],
      "metadata": {
        "id": "fF3oPXX3A31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP: Initialize and train a unidirectional LSTM model with pretrained GloVe embeddings\n",
        "Embeddings are fine-tuned during training. Model is trained for 10 epochs and validated after each epoch.\n"
      ],
      "metadata": {
        "id": "YBhuI2aaEr70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "uni_lstm, uni_optimizer = initialize_lstm(\n",
        "    pretrained_embedding=embedding_tensor,\n",
        "    freeze_embedding=False,\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=128,\n",
        "    output_dim=2,\n",
        "    n_layers=2,\n",
        "    bidirectional=False,\n",
        "    lstm_dropout=0.2,\n",
        "    fc_dropout=0.5,\n",
        "    learning_rate=0.25\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "train_lstm(uni_lstm, uni_optimizer, train_loader, val_loader, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5P8QmqbA54i",
        "outputId": "268dbd29-065a-42e8-fb4d-78d3253a32af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch | Train Loss  | Val Loss   | Val Acc  | Time(s)\n",
            "------------------------------------------------------------\n",
            "  1   |   0.6756   |   0.6192   |  65.02   |  7.63  \n",
            "  2   |   0.6366   |   0.6602   |  55.56   |  6.05  \n",
            "  3   |   0.6200   |   0.6252   |  59.00   |  6.34  \n",
            "  4   |   0.6081   |   0.6423   |  57.64   |  6.59  \n",
            "  5   |   0.5958   |   0.6252   |  61.08   |  6.38  \n",
            "  6   |   0.5798   |   0.6231   |  58.95   |  6.14  \n",
            "  7   |   0.5689   |   0.5671   |  65.43   |  6.41  \n",
            "  8   |   0.5578   |   0.5399   |  67.80   |  6.18  \n",
            "  9   |   0.5476   |   0.5296   |  70.28   |  6.48  \n",
            " 10   |   0.5385   |   0.5330   |  69.06   |  6.25  \n",
            "\n",
            " Best Val Acc: 70.28%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Uni-LSTM Performance Summary:\n",
        " - Best validation accuracy: **70.28%** (Epoch 9)\n",
        " - Gradual improvement over epochs shows the model is learning well\n",
        " - Slight overfitting in early epochs was corrected with steady training\n",
        "\n",
        "#  Next Step:\n",
        " Try a Bi-LSTM to see if capturing context in both directions improves performance further.\n"
      ],
      "metadata": {
        "id": "RXpbtqOUExTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP: Initialize and train a Bi-directional LSTM model using pretrained GloVe embeddings\n",
        " Bi-directional LSTM captures context from both past and future tokens.\n",
        "Embeddings are trainable. Training runs for 10 epochs with validation after each epoch.\n"
      ],
      "metadata": {
        "id": "HCzm7SLrE3yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Bi-directional LSTM\n",
        "bi_lstm, bi_optimizer = initialize_lstm(\n",
        "    pretrained_embedding=embedding_tensor,  # same fastText embeddings\n",
        "    freeze_embedding=False,                 # allow embeddings to be fine-tuned\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=128,\n",
        "    output_dim=2,\n",
        "    n_layers=2,\n",
        "    bidirectional=True,                     # This enables bi-directional LSTM\n",
        "    lstm_dropout=0.2,\n",
        "    fc_dropout=0.5,\n",
        "    learning_rate=0.25\n",
        ")\n",
        "\n",
        "# Train Bi-LSTM\n",
        "train_lstm(bi_lstm, bi_optimizer, train_loader, val_loader, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W8NekOLBNQ2",
        "outputId": "8792c42c-f07c-44eb-9d9b-2585b5bb84ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch | Train Loss  | Val Loss   | Val Acc  | Time(s)\n",
            "------------------------------------------------------------\n",
            "  1   |   0.6695   |   0.6283   |  61.34   |  11.09 \n",
            "  2   |   0.6189   |   0.5891   |  62.40   |  10.92 \n",
            "  3   |   0.5992   |   0.6210   |  59.59   |  11.12 \n",
            "  4   |   0.5898   |   0.5216   |  70.73   |  11.25 \n",
            "  5   |   0.5786   |   0.4905   |  74.26   |  11.30 \n",
            "  6   |   0.5681   |   0.5589   |  66.05   |  11.20 \n",
            "  7   |   0.5588   |   0.6585   |  57.18   |  11.01 \n",
            "  8   |   0.5500   |   0.5350   |  68.50   |  10.96 \n",
            "  9   |   0.5393   |   0.6475   |  59.53   |  11.04 \n",
            " 10   |   0.5294   |   0.6213   |  61.02   |  11.07 \n",
            "\n",
            " Best Val Acc: 74.26%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Bi-LSTM Performance Summary:\n",
        "- Best validation accuracy: **74.26%** (Epoch 5)\n",
        " - Outperformed Uni-LSTM (70.28%) in capturing contextual features\n",
        " - Validation accuracy fluctuated a bit — possibly due to overfitting after epoch 5\n",
        " - Still showed stronger peak performance overall\n",
        "\n",
        "#  Takeaway:\n",
        " Bi-directional LSTM provides a richer understanding of sequence context,\n",
        " which can lead to improved classification accuracy — especially on tasks involving nuanced language.\n"
      ],
      "metadata": {
        "id": "GZ9wwgkfE81Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np # make sure numpy is imported\n",
        "\n",
        "def evaluate_model(model, dataloader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb = xb.to(device)\n",
        "            # Ensure preds is on the CPU before converting to NumPy\n",
        "            # Move the tensor to CPU before calling .numpy()\n",
        "            preds = model(xb).argmax(dim=1).cpu().detach().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "\n",
        "    acc  = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds)\n",
        "    rec  = recall_score(all_labels, all_preds)\n",
        "    f1   = f1_score(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1-score\": f1\n",
        "    }\n",
        "\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYl7_49HBpxa",
        "outputId": "2e18cb80-0b9c-4dca-846e-2caaca753fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP: Evaluate and compare Uni-LSTM vs Bi-LSTM on the test set\n",
        " Calculates accuracy, precision, recall, and F1-score for both models\n",
        " Outputs a side-by-side comparison table to assess performance\n"
      ],
      "metadata": {
        "id": "2nuTrycwFFIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports === #\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# === Device Configuration === #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# === Evaluation Function === #\n",
        "def evaluate_model(model, dataloader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            # Predict and move to CPU → .tolist() avoids numpy errors\n",
        "            preds = model(xb).argmax(dim=1).cpu().tolist()\n",
        "            labels = yb.cpu().tolist()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    acc  = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds)\n",
        "    rec  = recall_score(all_labels, all_preds)\n",
        "    f1   = f1_score(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1-score\": f1\n",
        "    }\n",
        "\n",
        "# === Evaluate Models === #\n",
        "# Ensure `uni_lstm` and `bi_lstm` are trained and on the correct device\n",
        "uni_results = evaluate_model(uni_lstm, test_loader, name=\"Uni-LSTM\")\n",
        "bi_results = evaluate_model(bi_lstm, test_loader, name=\"Bi-LSTM\")\n",
        "\n",
        "# === Extract Metrics === #\n",
        "uni_accuracy = uni_results[\"Accuracy\"]\n",
        "uni_precision = uni_results[\"Precision\"]\n",
        "uni_recall = uni_results[\"Recall\"]\n",
        "uni_f1 = uni_results[\"F1-score\"]\n",
        "\n",
        "bi_accuracy = bi_results[\"Accuracy\"]\n",
        "bi_precision = bi_results[\"Precision\"]\n",
        "bi_recall = bi_results[\"Recall\"]\n",
        "bi_f1 = bi_results[\"F1-score\"]\n",
        "\n",
        "# === Comparison Print Function === #\n",
        "def compare_lstm_models():\n",
        "    print(\"\\nModel Evaluation Results on Test Set\")\n",
        "    print(f\"{'Metric':<12}| {'Uni-LSTM':<10}| {'Bi-LSTM':<10}\")\n",
        "    print(\"-\" * 38)\n",
        "    print(f\"{'Accuracy':<12}| {uni_accuracy:.4f}    | {bi_accuracy:.4f}\")\n",
        "    print(f\"{'Precision':<12}| {uni_precision:.4f}    | {bi_precision:.4f}\")\n",
        "    print(f\"{'Recall':<12}| {uni_recall:.4f}    | {bi_recall:.4f}\")\n",
        "    print(f\"{'F1-Score':<12}| {uni_f1:.4f}    | {bi_f1:.4f}\")\n",
        "\n",
        "# === Run Comparison === #\n",
        "compare_lstm_models()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1CpCXfJCVaq",
        "outputId": "007854ce-da7f-49ec-ae8e-4f506990e411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Model Evaluation Results on Test Set\n",
            "Metric      | Uni-LSTM  | Bi-LSTM   \n",
            "--------------------------------------\n",
            "Accuracy    | 0.6955    | 0.6093\n",
            "Precision   | 0.3837    | 0.3381\n",
            "Recall      | 0.6167    | 0.7969\n",
            "F1-Score    | 0.4731    | 0.4748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Insights:\n",
        "\n",
        "Uni-LSTM achieved higher accuracy and precision, making it more reliable for correctly predicting positive labels without too many false alarms.\n",
        "\n",
        "Bi-LSTM had a significantly higher recall, meaning it was better at capturing most of the actual positive cases — even if it misclassified more negatives as positives.\n",
        "\n",
        "F1-score was nearly identical, suggesting a trade-off: Bi-LSTM is more inclusive, Uni-LSTM is more precise.\n",
        "\n",
        "\n",
        "If your task favors sensitivity (like detecting mental health risk where missing positives is costly), Bi-LSTM is the better choice.\n",
        "\n",
        "If you need precision (e.g., minimizing false positives in resource-limited settings), Uni-LSTM is more suitable."
      ],
      "metadata": {
        "id": "Om57kBrNFO1x"
      }
    }
  ]
}